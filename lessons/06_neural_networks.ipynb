{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial neural networks are models trying to imitate a biological neural network as we can see in the following figure:\n",
    "\n",
    "![Neural Network](../images/neural_networks.png)\n",
    "\n",
    "Neural networks have been became popular in the last decade since the great variety of taks that they can address, not just regression and classification challenges. Another reason is that computer are more powerful that a few decades ago, that implies we can work with more data (not only tabular) and also train more complex models.\n",
    "\n",
    "![NN Cat](../images/cat_nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a set of labeled points, with two categories (A and B). The goal is to construct a mapping from $\\mathbb{R}^2$ to $\\{A, B\\}$.\n",
    "\n",
    "![Example data](../images/example_dataset_nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The artificial neural network approach uses repeated application of a simple, nonlinear function. We will base our network on the sigmoid function:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "We may regard $\\sigma(x)$ as a smoothed version of a step function, which itself mimics the behavior of a neuron in the brain.\n",
    "\n",
    "The steepness and location of the transition in the sigmoid function may be altered by scaling and shifting the argument or, in the language of neural networks, by _weighting_ and _biasing_ the input. Let $a$ be a vector produced by the neurons in one layer, then the vector of outputs from the next layer has the form\n",
    "\n",
    "$$\n",
    "\\sigma \\left( W a + b\\right)\n",
    "$$\n",
    "\n",
    "where $W$ is the matrix of weights and $b$ the vector of $biases$. The number of columns\n",
    "in W matches the number of neurons that produced the vector a at the previous layer. The number of rows in $W$ matches the number of neurons at the current layer. The number of components in $b$ also matches the number of neurons at the current layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the following example of an artificial neural network with four layers:\n",
    "\n",
    "![Example NN with four layers](../images/nn_four_layers.png)\n",
    "\n",
    "Since the input data has the form $x \\in \\mathbb{R}^2$, the weights and biases for the second layer may be represented by a matrix $W^{[2]} \\in \\mathbb{R}^{2 \\times 2}$ and a vector $b^{[2]} \\in \\mathbb{R}^2$, respectively. The output from the second layer has the form\n",
    "\n",
    "$$\n",
    "a^{[2]} = \\sigma \\left( W^{[2]} x + b^{[2]} \\right) \\in \\mathbb{R}^2\n",
    "$$\n",
    "\n",
    "The third layer has three neurons, then $W^{[3]} \\in \\mathbb{R}^{3 \\times 2}$, $b^{[2]} \\in \\mathbb{R}^3$ and the output from the third layer is\n",
    "\n",
    "$$\n",
    "a^{[3]} = \\sigma \\left( W^{[3]} a^{[2]} + b^{[3]} \\right) =  \\sigma\\left(  W^{[3]} \\sigma \\left( W^{[2]} x + b^{[2]} \\right)  + b^{[3]} \\right)\\in \\mathbb{R}^3\n",
    "$$\n",
    "\n",
    "Finally, for the fourth (output) layer $W^{[4]} \\in \\mathbb{R}^{2 \\times 3}$ and $b^{[4]} \\in \\mathbb{R}^2$. The output of the overall network has the form\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "F(x)\n",
    "&= \\sigma \\left( W^{[4]} a^{[3]} + b^{[4]} \\right) \\\\\n",
    "&= \\sigma \\left( W^{[4]} \\sigma \\left( W^{[3]} a^{[2]} + b^{[3]} \\right)  + b^{[4]} \\right) \\\\\n",
    "&= \\sigma \\left( W^{[4]} \\sigma \\left( W^{[3]} \\sigma \\left( W^{[2]} x + b^{[2]} \\right) + b^{[3]} \\right)  + b^{[4]} \\right) \\in \\mathbb{R}^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that the input layer has to have two neurons since we are only working with a input with two features and the output layer also has two neurons but in this case is because there are only two categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This neural network define a function $F: \\mathbb{R}^2 \\to \\mathbb{R}^2$ in terms of its 23 parameters (entries in the wright matrices and bias vectors). Without loss of generality, we can encode the categories as vectors,\n",
    "\n",
    "$$\n",
    "A : \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n",
    "\\quad\n",
    "\\text{and}\n",
    "\\quad\n",
    "B : \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "We need to optimize over the 23 parameters in order to classify the inputs into categories A or B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layers in between the input and output layer are called hidden layers. There is no special meaning to this phrase; it simply indicates that these neurons are performing intermediate calculations. __Deep Learning__ is a loosely defined term which implies that many hidden layers are being used.\n",
    "\n",
    "The general setup consider $L$ layers, with layers 1 and $L$ being the input and output layers, respectiveley. Suppose that layer $l$, for $l=1, 2, \\ldots, L$ contains $n_l$ neurons. So $n_1$ is the dimension of the input data.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a^{[1]} &= x \\\\\n",
    "a^{[l]} &= \\sigma \\left( W^{[l]} a^{[l-1]} + b^{[l]} \\right) \\quad \\text{for } l=2, 3, \\ldots, L.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now suppose we have $N$ samples of training data,\n",
    "$$\n",
    "\\left\\{ x^{\\{i\\}} \\right\\}_{i=1}^N \\subset \\mathbb{R}^{n_1}\n",
    "$$\n",
    "for which there are given target outputs\n",
    "$$\n",
    "\\left\\{ y^{\\{i\\}} \\right\\}_{i=1}^N \\subset \\mathbb{R}^{n_L}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quadratic cost function that we widh to minimize hast the form\n",
    "\n",
    "$$\n",
    "J\\left(W, b\\right) = \\frac{1}{N} \\sum_{i=1}^N \\frac{1}{2} \\left\\lVert y^{\\{i\\}} - a^{[L]} x^{\\{i\\}} \\right\\rVert^2_2\n",
    "$$\n",
    "\n",
    "where $W$, $b$ are the set of all the weight matrices and biases vectors, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pinn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b495ef8c557e1213b070efa440d5756e52b9d742e01b39c369ce0fdb1e54097c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
