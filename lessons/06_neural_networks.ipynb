{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial neural networks are models trying to imitate a biological neural network as we can see in the following figure:\n",
    "\n",
    "![Neural Network](../images/neural_networks.png)\n",
    "\n",
    "Neural networks have been became popular in the last decade since the great variety of taks that they can address, not just regression and classification challenges. Another reason is that computer are more powerful that a few decades ago, that implies we can work with more data (not only tabular) and also train more complex models.\n",
    "\n",
    "![NN Cat](../images/cat_nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a set of labeled points, with two categories (A and B). The goal is to construct a mapping from $\\mathbb{R}^2$ to $\\{A, B\\}$.\n",
    "\n",
    "![Example data](../images/example_dataset_nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The artificial neural network approach uses repeated application of a simple, nonlinear function. We will base our network on the sigmoid function:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "We may regard $\\sigma(x)$ as a smoothed version of a step function, which itself mimics the behavior of a neuron in the brain.\n",
    "\n",
    "The steepness and location of the transition in the sigmoid function may be altered by scaling and shifting the argument or, in the language of neural networks, by _weighting_ and _biasing_ the input. Let $a$ be a vector produced by the neurons in one layer, then the vector of outputs from the next layer has the form\n",
    "\n",
    "$$\n",
    "\\sigma \\left( W a + b\\right)\n",
    "$$\n",
    "\n",
    "where $W$ is the matrix of weights and $b$ the vector of $biases$. The number of columns\n",
    "in W matches the number of neurons that produced the vector a at the previous layer. The number of rows in $W$ matches the number of neurons at the current layer. The number of components in $b$ also matches the number of neurons at the current layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the following example of an artificial neural network with four layers:\n",
    "\n",
    "![Example NN with four layers](../images/nn_four_layers.png)\n",
    "\n",
    "Since the input data has the form $x \\in \\mathbb{R}^2$, the weights and biases for the second layer may be represented by a matrix $W^{[2]} \\in \\mathbb{R}^{2 \\times 2}$ and a vector $b^{[2]} \\in \\mathbb{R}^2$, respectively. The output from the second layer has the form\n",
    "\n",
    "$$\n",
    "a^{[2]} = \\sigma \\left( W^{[2]} x + b^{[2]} \\right) \\in \\mathbb{R}^2\n",
    "$$\n",
    "\n",
    "The third layer has three neurons, then $W^{[3]} \\in \\mathbb{R}^{3 \\times 2}$, $b^{[2]} \\in \\mathbb{R}^3$ and the output from the third layer is\n",
    "\n",
    "$$\n",
    "a^{[3]} = \\sigma \\left( W^{[3]} a^{[2]} + b^{[3]} \\right) =  \\sigma\\left(  W^{[3]} \\sigma \\left( W^{[2]} x + b^{[2]} \\right)  + b^{[3]} \\right)\\in \\mathbb{R}^3\n",
    "$$\n",
    "\n",
    "Finally, for the fourth (output) layer $W^{[4]} \\in \\mathbb{R}^{2 \\times 3}$ and $b^{[4]} \\in \\mathbb{R}^2$. The output of the overall network has the form\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "F(x)\n",
    "&= \\sigma \\left( W^{[4]} a^{[3]} + b^{[4]} \\right) \\\\\n",
    "&= \\sigma \\left( W^{[4]} \\sigma \\left( W^{[3]} a^{[2]} + b^{[3]} \\right)  + b^{[4]} \\right) \\\\\n",
    "&= \\sigma \\left( W^{[4]} \\sigma \\left( W^{[3]} \\sigma \\left( W^{[2]} x + b^{[2]} \\right) + b^{[3]} \\right)  + b^{[4]} \\right) \\in \\mathbb{R}^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that the input layer has to have two neurons since we are only working with a input with two features and the output layer also has two neurons but in this case is because there are only two categories.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This neural network define a function $F: \\mathbb{R}^2 \\to \\mathbb{R}^2$ in terms of its 23 parameters (entries in the wright matrices and bias vectors). Without loss of generality, we can encode the categories as vectors,\n",
    "\n",
    "$$\n",
    "A : \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n",
    "\\quad\n",
    "\\text{and}\n",
    "\\quad\n",
    "B : \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "We need to optimize over the 23 parameters in order to classify the inputs into categories A or B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The layers in between the input and output layer are called hidden layers. There is no special meaning to this phrase; it simply indicates that these neurons are performing intermediate calculations. __Deep Learning__ is a loosely defined term which implies that many hidden layers are being used.\n",
    "\n",
    "The general setup consider $L$ layers, with layers 1 and $L$ being the input and output layers, respectiveley. Suppose that layer $l$, for $l=1, 2, \\ldots, L$ contains $n_l$ neurons. So $n_1$ is the dimension of the input data.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "a^{[1]} &= x \\\\\n",
    "a^{[l]} &= \\sigma \\left( W^{[l]} a^{[l-1]} + b^{[l]} \\right) \\quad \\text{for } l=2, 3, \\ldots, L.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now suppose we have $N$ samples of training data,\n",
    "$$\n",
    "\\left\\{ x^{\\{i\\}} \\right\\}_{i=1}^N \\subset \\mathbb{R}^{n_1}\n",
    "$$\n",
    "for which there are given target outputs\n",
    "$$\n",
    "\\left\\{ y^{\\{i\\}} \\right\\}_{i=1}^N \\subset \\mathbb{R}^{n_L}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quadratic cost function that we widh to minimize hast the form\n",
    "\n",
    "$$\n",
    "J\\left(W, b\\right) = \\frac{1}{N} \\sum_{i=1}^N \\frac{1}{2} \\left\\lVert y^{\\{i\\}} - a^{[L]} \\left(x^{\\{i\\}}\\right) \\right\\rVert^2_2\n",
    "$$\n",
    "\n",
    "where $W$, $b$ are the set of all the weight matrices and biases vectors, respectively.\n",
    "\n",
    "We can minimize the cost function with gradient descent as we already saw previously. Without loss of generality, consider $J: \\mathbb{R}^s \\to \\mathbb{R}$ and $\\theta \\in \\mathbb{R}^s$, then the algorithm has the following form\n",
    "\n",
    "$$\n",
    "\\theta \\to \\theta - \\alpha \\nabla J(\\theta)\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the learning rate and the partial derivative of the cost function is a sum over the training data of individual partial derivatives, more precisely\n",
    "\n",
    "$$\n",
    "\\nabla J(\\theta) = \\frac{1}{N} \\sum_{i=1}^N \\nabla C_{x^{\\{i\\}}}(\\theta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have a large number of parameters and a large number of training points, computing the gradient vector at every iteration of the steepest descent method can be prohibitively expensive. A cheaper alternative is __*Stochastic Gradient Descent*__ which has several variations, one of the most commons is to take a smaller set of training data in each iteration. For some $m \\ll N$ each iteration has the following form:\n",
    "\n",
    "1. Choose $m$ integers, $k_1, k_2, \\ldots, k_m$ uniformly at random from $\\{1, 2, \\ldots, N\\}$.\n",
    "2. Update \n",
    "$$\n",
    "\\theta \\to \\theta - \\alpha \\frac{1}{m} \\sum_{i=1}^m \\nabla C_{x^{\\{k_i\\}}}(\\theta)\n",
    "$$\n",
    "\n",
    "In this iteration, the set $\\{ x^{\\{k_i\\}} \\}_{i=1}^k$ is known as a _minibatch_. There is a without replacement alternative where, assuming $N = Km$ for some $K$, we split the training set randomly into $K$ distinct minibatches and cycle through them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task is to compute partial derivatives of the cost function with respect to each element of $W^{[l]}$ and $b^{[l]}$. We therefore focus our attention on computing those individual partial derivatives.\n",
    "\n",
    "Hence, for a fixed $i$-th training sample we want to compute the derivatives of $C_{x^{\\{i\\}}}$, so we may drop the dependence on $x^{\\{i\\}}$ and simply write\n",
    "\n",
    "$$\n",
    "C = \\frac{1}{2} \\left\\lVert y - a^{[L]}\\right\\rVert_2^2\n",
    "$$\n",
    "\n",
    "It is useful introduce two further sets of variables. First we let\n",
    "$$\n",
    "z^{[l]} = W^{[l]} a^{[l-1]} + b^{[l]} \\in \\mathbb{R}^{n_l} \\quad \\text{for} \\; l=2, 3, \\ldots, L.\n",
    "$$\n",
    "where $z_j^{[l]}$, the $j$-th component of $z_j^{[l]}$ is the _weighted input$ for neuron $j$ at layer $l$.\n",
    "\n",
    "Secondly,\n",
    "$$\n",
    "\\delta_j^{[l]} = \\dfrac{\\partial C}{\\partial z_j^{[l]}} \\in \\mathbb{R}^{n_l} \\quad \\text{for} \\; 1 \\leq j \\leq n_l \\quad \\text{and} \\quad 2 \\leq l \\leq L\n",
    "$$\n",
    "\n",
    "The output $a^{[L]}$ can be evaluated from a forward pass through the network, computing in order\n",
    "$$\n",
    "a^{[1]}, z^{[2]}, a^{[2]}, z^{[3]}, a^{[3]}, \\ldots, z^{[L]}, a^{[L]}\n",
    "$$\n",
    "Then,\n",
    "$$\n",
    "\\delta^{[L-1]}, \\delta^{[L-2]}, \\ldots, \\delta^{[2]}\n",
    "$$\n",
    "can be computed in backward pass such that \n",
    "$$\n",
    "\\dfrac{\\partial C}{\\partial W^{[l]}_{j, k}} = \\delta_j^{[l]} a_k^{[l-1]}\n",
    "\\quad\n",
    "\\text{and}\n",
    "\\quad\n",
    "\\dfrac{\\partial C}{\\partial \\beta^{[l]}_j} = \\delta^{[l]}_j\n",
    "\\quad\n",
    "\\text{for} \\; 1 \\leq j, k \\leq n_l \\quad \\text{and} \\quad 2 \\leq l \\leq L\n",
    "$$\n",
    "\n",
    "Computing gradients in this way is known as __*back propagation*__.\n",
    "\n",
    "Another good resource for understand back propagation is [this website](https://colah.github.io/posts/2015-08-Backprop/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a classification task with the same MNIST dataset but using PyTorch. Nowadyays, there are many Neural Network / Deep Learning frameworks availables, such as Tensorflow, Keras, JAX, Matlab, etc. We decided to use PyTorch since it has a really friendly user API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need the data. The way to access to it is different to scikit-learn, but it is essentialy the same but we load train and test sets in a separate way. Also, since the elements are images it is necessary to cast them to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some samples again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(label)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DataLoader` is a tool for handle batches and iterate over the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the size of each batch, in particular `X`-batches are made of only one channel and 28$\\times$28 pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating our model we need to select the device where PyTorch is going to work on. By default tensors are created on the CPU, but with the following line we can use the GPU (if it is available).\n",
    "\n",
    "__Remark:__ If you running this notebook on Google Colab you need to enable GPU manually. Go to Menu `Menu > Runtime > Change Runtime` and change hardware accelaration to __GPU__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a neural network in PyTorch, we create a class that inherits from `nn.Module`. We define the layers of the network in the `__init__` function and specify how data will pass through the network in the forward function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just created a fully connected neural network made of three layers (just one hidden layer). Note the activation function is not a logistic one, but another non-linear function calle _ReLU_ (Rectified Linear Unit) defined as \n",
    "\n",
    "$$\n",
    "ReLu(x) = \\max(0, x)\n",
    "$$\n",
    "\n",
    "![ReLU](../images/ReLU.png)\n",
    "\n",
    "[Source](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to choose a loss (cost) function and the optimizer. \n",
    "\n",
    "- Cross Entropy is useful when training a classification problem with several classes. In this cases, 10 digits.\n",
    "- Stochastic Gradient Descent with a learning rate of 0.001 (it is a standard practice to use this value as first guest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and backpropagates the prediction error to adjust the model’s parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also check the model’s performance against the test dataset to ensure it is learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we have to decide the number of _epochs_ and just train the data. Sometimes epochs and batch size confuse people.\n",
    "\n",
    "- The batch size is a hyperparameter of gradient descent that controls the number of __training samples__ to work through before the model’s internal parameters are updated.\n",
    "- The number of epochs is a hyperparameter of gradient descent that controls the number of __complete passes through the training dataset__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.307160  [    0/60000]\n",
      "loss: 2.298913  [ 6400/60000]\n",
      "loss: 2.302065  [12800/60000]\n",
      "loss: 2.288452  [19200/60000]\n",
      "loss: 2.291320  [25600/60000]\n",
      "loss: 2.284930  [32000/60000]\n",
      "loss: 2.279185  [38400/60000]\n",
      "loss: 2.287402  [44800/60000]\n",
      "loss: 2.261695  [51200/60000]\n",
      "loss: 2.265147  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 24.6%, Avg loss: 2.260971 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.260471  [    0/60000]\n",
      "loss: 2.248112  [ 6400/60000]\n",
      "loss: 2.262750  [12800/60000]\n",
      "loss: 2.227724  [19200/60000]\n",
      "loss: 2.243618  [25600/60000]\n",
      "loss: 2.237414  [32000/60000]\n",
      "loss: 2.218070  [38400/60000]\n",
      "loss: 2.240417  [44800/60000]\n",
      "loss: 2.200825  [51200/60000]\n",
      "loss: 2.200368  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 48.5%, Avg loss: 2.196969 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.195424  [    0/60000]\n",
      "loss: 2.174558  [ 6400/60000]\n",
      "loss: 2.207611  [12800/60000]\n",
      "loss: 2.137497  [19200/60000]\n",
      "loss: 2.170787  [25600/60000]\n",
      "loss: 2.162538  [32000/60000]\n",
      "loss: 2.121905  [38400/60000]\n",
      "loss: 2.162595  [44800/60000]\n",
      "loss: 2.102334  [51200/60000]\n",
      "loss: 2.093725  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Avg loss: 2.089809 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.086431  [    0/60000]\n",
      "loss: 2.049990  [ 6400/60000]\n",
      "loss: 2.111760  [12800/60000]\n",
      "loss: 1.989184  [19200/60000]\n",
      "loss: 2.041787  [25600/60000]\n",
      "loss: 2.027561  [32000/60000]\n",
      "loss: 1.958616  [38400/60000]\n",
      "loss: 2.029515  [44800/60000]\n",
      "loss: 1.933357  [51200/60000]\n",
      "loss: 1.915464  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 66.8%, Avg loss: 1.906900 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.906968  [    0/60000]\n",
      "loss: 1.845718  [ 6400/60000]\n",
      "loss: 1.938427  [12800/60000]\n",
      "loss: 1.757425  [19200/60000]\n",
      "loss: 1.813801  [25600/60000]\n",
      "loss: 1.795088  [32000/60000]\n",
      "loss: 1.705639  [38400/60000]\n",
      "loss: 1.815095  [44800/60000]\n",
      "loss: 1.671603  [51200/60000]\n",
      "loss: 1.647297  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.2%, Avg loss: 1.627216 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pinn-backup')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "35923595c379b2f8a43fe857c5f1ff97e2ec11b937ce0326cd8f57467725d828"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
